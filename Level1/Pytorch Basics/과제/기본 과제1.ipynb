{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본 과제 1번"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 연산\n",
    "- 피연산자가 중 하나라도 tensor 타입이면 결과는 tensor로 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n",
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.Tensor([5])\n",
    "B = torch.Tensor([7])\n",
    "\n",
    "print(torch.add(A, B))\n",
    "print(A + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 인덱싱, 슬라이싱, 조인 등..\n",
    "### ```torch.index_select()```\n",
    "- torch.index_select(input, dim, index, *, out=None)\n",
    "- index 범위를 따로 만들고 index_select 함수안에 파라미터로 넣는게 편함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [3.]])\n",
      "1차원 벡터로 변환: tensor([1., 3.])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "# TODO : [1, 3]을 만드세요!\n",
    "\n",
    "# torch.index_select 함수를 써서 해보세요!\n",
    "indices = torch.tensor([0])\n",
    "output = torch.index_select(A, 1, indices)\n",
    "\n",
    "print(output)\n",
    "print('1차원 벡터로 변환:', output.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```torch.gather()```\n",
    "- torch.gather(input, dim, index, *, sparse_grad=False, out=None)\n",
    "- 2차원, 3차원... 등에 사용\n",
    "- index는 input과 차원이 같아야한다!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim=1 이니까 열 방향으로 indexing 진행\n",
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4.])\n",
      "----\n",
      "tensor([[1., 4.],\n",
      "        [1., 4.]])\n",
      "tensor([1., 4.])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "# 1차원으로 줄이고 인덱싱도 가능\n",
    "B = A.view(-1)\n",
    "print(torch.gather(B, 0, torch.tensor([0,3])))\n",
    "print('----')\n",
    "\n",
    "# torch.gather 함수를 써서 해보세요!\n",
    "# A에서 0(행방향)으로 인덱싱\n",
    "output = torch.gather(A, 0, torch.tensor([[0,1], [0,1]]))\n",
    "# print(output)\n",
    "print(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 고난도, 3D tensor gather -> 2차원\n",
    "\n",
    "# TODO : 임의의 크기의 3D tensor에서 대각선 요소 가져와 2D로 반환하는 함수를 만드세요! \n",
    "def get_diag_element_3D(A):\n",
    "\n",
    "    c = A.shape[0]\n",
    "    h = A.shape[1]\n",
    "    w = A.shape[2]\n",
    "\n",
    "    # 행과 열 중 어디가 기느냐에 따라 대각요소를 뽑는 범위가 달라진다. (더 짧은 쪽에 맞춰서 뽑아야 함)\n",
    "    if h <= w:\n",
    "        ind = torch.arange(h).expand(c,h).reshape(c,1,h)\n",
    "        output = torch.gather(A, 1, ind).view(-1,h)\n",
    "    else:\n",
    "        ind = torch.arange(w).expand(c,w).reshape(c,1,w)\n",
    "        output = torch.gather(A, 1, ind).view(-1,w)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  7, 13],\n",
       "        [16, 22, 28]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.Tensor([[[1, 2, 3],\n",
    "                   [4, 5, 6]]])\n",
    "print(get_diag_element_3D(A))\n",
    "\n",
    "A = torch.tensor([[[ 1,  2,  3,  4,  5],\n",
    "                   [ 6,  7,  8,  9, 10],\n",
    "                   [11, 12, 13, 14, 15]],\n",
    "          \n",
    "                  [[16, 17, 18, 19, 20],\n",
    "                   [21, 22, 23, 24, 25],\n",
    "                   [26, 27, 28, 29, 30]]])\n",
    "get_diag_element_3D(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```torch.chunk()```\n",
    "- torch.chunk(input, chunks, dim=0)\n",
    "- tensor를 split 해주는 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1, 2, 3]]), tensor([[4, 5, 6]]))\n",
      "(tensor([[1, 2],\n",
      "        [4, 5]]), tensor([[3],\n",
      "        [6]]))\n",
      "(tensor([[1],\n",
      "        [4]]), tensor([[2],\n",
      "        [5]]), tensor([[3],\n",
      "        [6]]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#  torch.chunk\n",
    "t = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "print(torch.chunk(t, 2, 0)) # 행 방향으로 2등분\n",
    "print(torch.chunk(t, 2, 1)) # 열 방향으로 2등분\n",
    "print(torch.chunk(t, 3, 1)) # 열 방향으로 3등분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```torch.swapdims()```\n",
    "- torch.swapdims(input, dim0, dim1)\n",
    "- torch.transpose()과 같은 기능이라 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n",
    "print(torch.swapdims(x, 0, 1))\n",
    "print(torch.swapdims(x, 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```torch.scatter()``` ???\n",
    "- Tensor.scatter_(dim, index, src, reduce=None)\n",
    "- 공식문서 : https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 4, 0],\n",
      "        [0, 2, 0, 0, 0],\n",
      "        [0, 0, 3, 0, 0]])\n",
      "tensor([[1, 2, 3, 0, 0],\n",
      "        [6, 7, 0, 0, 8],\n",
      "        [0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "src = torch.arange(1, 11).reshape((2, 5))\n",
    "\n",
    "index = torch.tensor([[0, 1, 2, 0]])\n",
    "print(torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src))\n",
    "\n",
    "index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
    "print(torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```torch.einsum()``` ???\n",
    "- torch.einsum(equation, *operands)\n",
    "- 공식문서: https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4996, -1.3301,  0.3208, -0.5237],\n",
      "        [-0.2794,  0.7438, -0.1794,  0.2929],\n",
      "        [-0.2183,  0.5810, -0.1401,  0.2288],\n",
      "        [ 0.0606, -0.1612,  0.0389, -0.0635],\n",
      "        [-0.4102,  1.0921, -0.2634,  0.4300]])\n",
      "tensor([[[ 1.4559, -0.0908,  3.8621, -0.3544],\n",
      "         [-1.8633,  0.9283, -2.3702, -1.8273]],\n",
      "\n",
      "        [[ 2.4102, -4.2638, -6.2387,  6.2946],\n",
      "         [ 0.6687, -2.3406, -2.4810,  2.6455]],\n",
      "\n",
      "        [[ 2.5591,  2.6877, -4.0414, -3.2840],\n",
      "         [-0.7194,  1.5803,  3.4003,  1.6086]]])\n"
     ]
    }
   ],
   "source": [
    "torch.einsum('ii', torch.randn(4, 4))\n",
    "\n",
    "torch.einsum('ii->i', torch.randn(4, 4))\n",
    "\n",
    "x = torch.randn(5)\n",
    "y = torch.randn(4)\n",
    "print(torch.einsum('i,j->ij', x, y))\n",
    "\n",
    "As = torch.randn(3,2,5)\n",
    "Bs = torch.randn(3,5,4)\n",
    "print(torch.einsum('bij,bjk->bik', As, Bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Module 클래스\n",
    "### ```torch.nn.Module```\n",
    "- 모든 신경망 모듈의 기본 클래스입니다.\n",
    "- 모듈은 트리 구조에 중첩할 수 있도록 다른 모듈을 포함할 수도 있습니다. 하위 모듈을 일반 특성으로 할당할 수 있습니다.\n",
    "- 결국 기능들을 한 곳에 모아놓는 상자 역할을 한다.\n",
    "- 내부에 여러가지 method들을 사용할 수 있다.\n",
    "- ex) add_module, apply, buffers, children, hook, forward 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__() ### init 과정에 반드시 super 코드가 들어감\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```nn.Sequental()```\n",
    "- 모듈을 하나로 묶어서 순차적으로 실행시킬 수 있다\n",
    "- ```nn.ModuleList```은 말 그대로 모듈을 리스트에 저장만 함\n",
    "- ```nn.ModuleDict```은 말 그대로 모듈을 딕셔너리에 저장만 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합: tensor([11])\n"
     ]
    }
   ],
   "source": [
    "# TODO : 다음의 모듈(Module)을 읽고 이해해보세요!\n",
    "class Add(nn.Module):\n",
    "    def __init__(self, value):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "#         print(self.value)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x, self.value)\n",
    "        return x + self.value\n",
    "\n",
    "# TODO : 위에 모듈(Module)과 nn.Sequential를 이용해서\n",
    "#        입력값 x가 주어지면 다음의 연산을 처리하는 모델을 만들어보세요!\n",
    "#        y = x + 3 + 2 + 5\n",
    "calculator = nn.Sequential(Add(3), Add(2), Add(5))\n",
    "print('합:', calculator(torch.tensor([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```딥러닝 모델 만들기```\n",
    "- 최소 기능 단위인 Function을 만들고\n",
    "- Function들로 이루어진 Layer을 만들고\n",
    "- Layer들로 하나의 큰 Model을 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Function A Initialized\n",
      "        Function B Initialized\n",
      "    Layer AB Initialized\n",
      "        Function C Initialized\n",
      "        Function D Initialized\n",
      "    Layer CD Initialized\n",
      "Model ABCD Initialized\n",
      "\n",
      "Model ABCD started\n",
      "    Layer AB started\n",
      "        Function A started\n",
      "        Function A done\n",
      "        Function B started\n",
      "        Function B done\n",
      "    Layer AB done\n",
      "    Layer CD started\n",
      "        Function C started\n",
      "        Function C done\n",
      "        Function D started\n",
      "        Function D done\n",
      "    Layer CD done\n",
      "Model ABCD done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Function\n",
    "class Function_A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(f\"        Function A Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"        Function A started\")\n",
    "        print(f\"        Function A done\")\n",
    "\n",
    "class Function_B(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(f\"        Function B Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"        Function B started\")\n",
    "        print(f\"        Function B done\")\n",
    "\n",
    "class Function_C(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(f\"        Function C Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"        Function C started\")\n",
    "        print(f\"        Function C done\")\n",
    "\n",
    "class Function_D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(f\"        Function D Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"        Function D started\")\n",
    "        print(f\"        Function D done\")\n",
    "\n",
    "\n",
    "# Layer\n",
    "class Layer_AB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.a = Function_A()\n",
    "        self.b = Function_B()\n",
    "\n",
    "        print(f\"    Layer AB Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"    Layer AB started\")\n",
    "        self.a(x)\n",
    "        self.b(x)\n",
    "        print(f\"    Layer AB done\")\n",
    "\n",
    "class Layer_CD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c = Function_C()\n",
    "        self.d = Function_D()\n",
    "\n",
    "        print(f\"    Layer CD Initialized\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"    Layer CD started\")\n",
    "        self.c(x)\n",
    "        self.d(x)\n",
    "        print(f\"    Layer CD done\")\n",
    "\n",
    "\n",
    "# Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ab = Layer_AB()\n",
    "        self.cd = Layer_CD()\n",
    "\n",
    "        print(f\"Model ABCD Initialized\\n\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Model ABCD started\")\n",
    "        self.ab(x)\n",
    "        self.cd(x)\n",
    "        print(f\"Model ABCD done\\n\")\n",
    "\n",
    "\n",
    "x = torch.tensor([7])\n",
    "\n",
    "model = Model()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Parameter```\n",
    "- 선형회귀 ```Y = XW + b```에서 W와 b가 파라미터가 된다.\n",
    "- torch.nn.parameter.Parameter(data=None, requires_grad=True)\n",
    "- Parameter로 만들어야 W와 b가 저장이 된다.\n",
    "- ```state_dict()```를 통해 모델의 파라미터들을 확인할 수 있음\n",
    "- 하지만 대부분 torch.nn에 구현된 레이어를 사용하기 때문에 직접 지정할 일은 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "# 예시\n",
    "# 1로 초기화하기 위해 torch.ones()를 사용했음\n",
    "out_features, in_features = 10, 5\n",
    "self.W = Parameter(torch.ones((out_features, in_features)))\n",
    "self.b = Parameter(torch.ones((out_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 모델 분석하기\n",
    "#### [위에서 만든 딥러닝 모델을 갖고 사용함](#딥러닝-모델-만들기)\n",
    "- 모델에서 어떤 module이 있는지 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (ab): Layer_AB(\n",
       "    (a): Function_A()\n",
       "    (b): Function_B()\n",
       "  )\n",
       "  (cd): Layer_CD(\n",
       "    (c): Function_C()\n",
       "    (d): Function_D()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```named_children```\n",
    "- 어떤 모듈(module)들이 있는지 보여줌\n",
    "- 대신, 한 단계 아래의 submodule까지만 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Name ] : ab\n",
      "[ Children ]\n",
      "Layer_AB(\n",
      "  (a): Function_A()\n",
      "  (b): Function_B()\n",
      ")\n",
      "------------------------------\n",
      "[ Name ] : cd\n",
      "[ Children ]\n",
      "Layer_CD(\n",
      "  (c): Function_C()\n",
      "  (d): Function_D()\n",
      ")\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, child in model.named_children():\n",
    "    print(f\"[ Name ] : {name}\\n[ Children ]\\n{child}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```named_modules```\n",
    "- 어떤 모듈(module)들이 있는지 보여줌\n",
    "- ```named_children```과 달리 자신에게 속하는 모든 submodule을 보여줌\n",
    "- 예를 들어, self.a = Function_A() 라면 Function_A 모듈의 name이 'a' 가 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Name ] : \n",
      "[ Module ]\n",
      "Model(\n",
      "  (ab): Layer_AB(\n",
      "    (a): Function_A()\n",
      "    (b): Function_B()\n",
      "  )\n",
      "  (cd): Layer_CD(\n",
      "    (c): Function_C()\n",
      "    (d): Function_D()\n",
      "  )\n",
      ")\n",
      "------------------------------\n",
      "[ Name ] : ab\n",
      "[ Module ]\n",
      "Layer_AB(\n",
      "  (a): Function_A()\n",
      "  (b): Function_B()\n",
      ")\n",
      "------------------------------\n",
      "[ Name ] : ab.a\n",
      "[ Module ]\n",
      "Function_A()\n",
      "------------------------------\n",
      "[ Name ] : ab.b\n",
      "[ Module ]\n",
      "Function_B()\n",
      "------------------------------\n",
      "[ Name ] : cd\n",
      "[ Module ]\n",
      "Layer_CD(\n",
      "  (c): Function_C()\n",
      "  (d): Function_D()\n",
      ")\n",
      "------------------------------\n",
      "[ Name ] : cd.c\n",
      "[ Module ]\n",
      "Function_C()\n",
      "------------------------------\n",
      "[ Name ] : cd.d\n",
      "[ Module ]\n",
      "Function_D()\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(f\"[ Name ] : {name}\\n[ Module ]\\n{module}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```get_submodule```\n",
    "- 모델 안에 있는 모듈 중, 특정 모듈만 가져오고 싶음\n",
    "- 모듈의 name을 사용해서 특정 모듈을 가져올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function_A 모듈을 가져오기\n",
    "# a를 가져와야하는데 a는 ab에 속한다.\n",
    "submodule = model.get_submodule('ab.a')\n",
    "print(submodule)\n",
    "print(submodule.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```named_parameters```\n",
    "- 각 모듈들의 파라미터를 반환해준다\n",
    "- weight와 bias 의 값을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.named_parameters():\n",
    "    print(i)\n",
    "\n",
    "# ㅇ\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(f\"[ Name ] : {name}\\n[ Parameter ]\\n{parameter}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```get_parameter```\n",
    "- ```get_submodule```과 동일하게 사용한다. (모듈의 name을 사용함)\n",
    "- 타겟의 파라미터 값을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ab 안의 b의 Weight1 값을 가져옴\n",
    "parameter = model.get_parameter('ab.b.W1')\n",
    "parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
