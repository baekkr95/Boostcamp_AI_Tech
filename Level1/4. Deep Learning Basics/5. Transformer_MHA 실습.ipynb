{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsnoyeYbS7i6"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/sjchoi86/upstage-basic-deeplearning/blob/main/notebook/mha.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/sjchoi86/upstage-basic-deeplearning/blob/main/notebook/mha.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View Source</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRwj6fIzVc-s"
   },
   "source": [
    "# Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6906,
     "status": "ok",
     "timestamp": 1644296646187,
     "user": {
      "displayName": "백경륜",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06459579199878379001"
     },
     "user_tz": -540
    },
    "id": "G9N9n94EVWy9",
    "outputId": "ebcb2aed-58e0-497f-b21a-05316cb9d4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.10.0+cu111].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRRvnKZ0XlJT"
   },
   "source": [
    "### Scaled Dot-Product Attention (SDPA)\n",
    "- Data $X \\in \\mathbb{R}^{n \\times d}$ where $n$ is the number data and $d$ is the data dimension\n",
    "- Query and Key $Q, K \\in \\mathbb{R}^{n \\times d_K}$ \n",
    "- Value $V \\in \\mathbb{R}^{n \\times d_V} $\n",
    "\n",
    "$\\text{Attention}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}} \\right)V \\in \\mathbb{R}^{n \\times d_V} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1644298673354,
     "user": {
      "displayName": "백경륜",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06459579199878379001"
     },
     "user_tz": -540
    },
    "id": "K-Z3Vd_VV5Pm",
    "outputId": "d1eaf493-7b0c-4e23-eb2e-86e421ea2d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDPA: Q[3, 30, 128] K[3, 50, 128] V[3, 50, 256] => out[3, 30, 256] attention[3, 30, 50]\n",
      "(Multi-Headed) SDPA: Q[3, 5, 30, 128] K[3, 5, 50, 128] V[3, 5, 50, 256] => out[3, 5, 30, 256] attention[3, 5, 30, 50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# ScaledDotProductAttention은 하나의 attention이다\n",
    "# 이것을 Multi head로 확장하는 것이 목표\n",
    "\n",
    "# N개의 embedding 된 word에 대해서 D차원의 벡터를\n",
    "# D_k의 Query와 Key 벡터들을 찾고 D_v의 Value 벡터들을 찾은 다음에\n",
    "# Attention(Q,K,V)을 하는 것\n",
    "# 최종적으로 나오는 것은 N개의 word에 대해서 각각 Value 벡터의 차원만큼 나온다.\n",
    "# Query 벡터에 대해서 인코드를 찾는 것이 목표\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def forward(self,Q,K,V,mask=None):\n",
    "        d_K = K.size()[-1] # key dimension (query dimension과 동일)\n",
    "\n",
    "        # Attention(Q,K,V) 수식에서 소프트맥스 안에 들어있는 수식(normalize까지 한 단계)\n",
    "        scores = Q.matmul(K.transpose(-2,-1))//np.sqrt(d_K)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "        # 소프트맥스를 통해서 attention이 나온다   \n",
    "        attention = F.softmax(scores,dim=-1)\n",
    "        # attention을 Value 벡터와 weighted sum을 하면, 최종적인 각 단어에 대한 인코딩 벡터(z)가 된다.\n",
    "        out = attention.matmul(V)\n",
    "        return out,attention\n",
    "\n",
    "# Demo run of scaled dot product attention \n",
    "SPDA = ScaledDotProductAttention()\n",
    "n_batch,d_K,d_V = 3,128,256 # d_K(=d_Q) does not necessarily be equal to d_V\n",
    "n_Q,n_K,n_V = 30,50,50  # query, key, value 벡터의 개수\n",
    "\n",
    "### query 벡터의 입력개수와 key, value 벡터의 입력개수가 달라도 된다??\n",
    "# 인코더, 디코더 attention에서는 다른 sequence 길이가 있어도 서로의 interaction을 구할 수 있기 때문\n",
    "# 디코더에 들어가는 입력들에 대해서 query들이 만들어지고\n",
    "# Key와 Value 벡터는 인코더에서 보내주기 때문에 입력 개수가 달라져도 된다\n",
    "\n",
    "\n",
    "Q = torch.rand(n_batch,n_Q,d_K)\n",
    "K = torch.rand(n_batch,n_K,d_K)\n",
    "V = torch.rand(n_batch,n_V,d_V)\n",
    "out,attention = SPDA.forward(Q,K,V,mask=None)\n",
    "\n",
    "# 차원 확인\n",
    "def sh(x): return str(x.shape)[11:-1] \n",
    "print (\"SDPA: Q%s K%s V%s => out%s attention%s\"%\n",
    "       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))\n",
    "\n",
    "# It supports 'multi-headed' attention\n",
    "n_batch,n_head,d_K,d_V = 3,5,128,256\n",
    "n_Q,n_K,n_V = 30,50,50 # n_K and n_V should be the same\n",
    "Q = torch.rand(n_batch,n_head,n_Q,d_K)\n",
    "K = torch.rand(n_batch,n_head,n_K,d_K)\n",
    "V = torch.rand(n_batch,n_head,n_V,d_V)\n",
    "out,attention = SPDA.forward(Q,K,V,mask=None)\n",
    "# out: [n_batch x n_head x n_Q x d_V]\n",
    "# attention: [n_batch x n_head x n_Q x n_K] \n",
    "\n",
    "# 차원 확인\n",
    "def sh(x): return str(x.shape)[11:-1] \n",
    "print (\"(Multi-Headed) SDPA: Q%s K%s V%s => out%s attention%s\"%\n",
    "       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PM1sAoh711rc"
   },
   "source": [
    "- SDPA의 출력물(out)의 shape을 확인해보면, (3 * 30 * 256)\n",
    "- Batch size인 3, Query 벡터들에 대한 30개, Value 벡터의 차원인 256이 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLbi13pDi3No"
   },
   "source": [
    "### Multi-Headed Attention (MHA)\n",
    "\n",
    "$\\text{head}_{\\color{red}i} = \\text{Attention}(Q {\\color{green}W}^Q_{\\color{red}i},K {\\color{green}W}^K_{\\color{red}i}, V {\\color{green}W}^V_{\\color{red}i}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1644299758887,
     "user": {
      "displayName": "백경륜",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06459579199878379001"
     },
     "user_tz": -540
    },
    "id": "Hf7j24l1dnSF",
    "outputId": "a893720b-2cd4-4d2e-c3bb-173b258abdbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input src:\t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
      "\n",
      "Q_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
      "K_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
      "V_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
      "\n",
      "Q_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
      "K_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
      "V_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
      "\n",
      "scores:   \t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n",
      "attention:\t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n",
      "\n",
      "x_raw:    \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n",
      "x_rsh1:   \t[128, 32, 5, 40]  \t= [n_batch, n_src, n_head, d_head]\n",
      "x_rsh2:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n",
      "\n",
      "Output x: \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,d_feat=128,n_head=5,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=None):\n",
    "        \"\"\"\n",
    "        :param d_feat: feature dimension\n",
    "        :param n_head: number of heads\n",
    "        :param actv: activation after each linear layer\n",
    "        :param USE_BIAS: whether to use bias\n",
    "        :param dropout_p: dropout rate, attention weight에 들어간다\n",
    "        :device: which device to use (e.g., cuda:0)\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention,self).__init__()\n",
    "        # feature dimension이 head의 개수로 나눠질 수 있어야 한다.\n",
    "        # transformer 요약에서 논문 이론과 구현체 차이점이 여기서 나옴\n",
    "        if (d_feat%n_head) != 0:\n",
    "            raise ValueError(\"d_feat(%d) should be divisible by b_head(%d)\"%(d_feat,n_head)) \n",
    "\n",
    "        self.d_feat = d_feat\n",
    "        self.n_head = n_head\n",
    "        self.d_head = self.d_feat // self.n_head\n",
    "        self.actv = actv\n",
    "        self.USE_BIAS = USE_BIAS\n",
    "        self.dropout_p = dropout_p # prob. of zeroed\n",
    "\n",
    "        # 임의의 embedding vector가 들어올 때, Query, Key, Value를 얻는 네트워크와\n",
    "        # 추가로 한번 더 가공해주는 output dimension\n",
    "        # 총 4개의 네트워크가 Multi head attention에 들어간다.\n",
    "        self.lin_Q = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
    "        self.lin_K = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
    "        self.lin_V = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
    "        self.lin_O = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "    \n",
    "    def forward(self,Q,K,V,mask=None):\n",
    "        \"\"\"\n",
    "        :param Q: [n_batch, n_Q, d_feat]\n",
    "        :param K: [n_batch, n_K, d_feat]\n",
    "        :param V: [n_batch, n_V, d_feat] <= n_K and n_V must be the same \n",
    "        :param mask: \n",
    "        \"\"\"\n",
    "        n_batch = Q.shape[0]\n",
    "        Q_feat = self.lin_Q(Q) \n",
    "        K_feat = self.lin_K(K) \n",
    "        V_feat = self.lin_V(V)\n",
    "        # Q_feat: [n_batch, n_Q, d_feat]\n",
    "        # K_feat: [n_batch, n_K, d_feat]\n",
    "        # V_feat: [n_batch, n_V, d_feat]\n",
    "\n",
    "        # 이 부분이 논문과 다름\n",
    "        # 뉴럴 네트워크를 통과해서 나온 feature를 K개로 조각을 내준다.\n",
    "        # Multi-head split of Q, K, and V (d_feat = n_head*d_head)\n",
    "        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
    "        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
    "        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n",
    "        # 차원을 확인해보면, d_feat이 n_head와 d_head로 나뉨\n",
    "        # Q_split: [n_batch, n_head, n_Q, d_head]\n",
    "        # K_split: [n_batch, n_head, n_K, d_head]\n",
    "        # V_split: [n_batch, n_head, n_V, d_head]\n",
    "\n",
    "        # Multi-Headed Attention\n",
    "        d_K = K.size()[-1] # key dimension\n",
    "        scores = torch.matmul(Q_split, K_split.permute(0,1,3,2)) / np.sqrt(d_K)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0,-1e9)\n",
    "\n",
    "        attention = torch.softmax(scores,dim=-1)\n",
    "        \n",
    "        # 여기서 single attention과 다르게, dropout이 들어간다.\n",
    "        # 논문에 없는 부분\n",
    "        x_raw = torch.matmul(self.dropout(attention),V_split) # dropout is NOT mentioned in the paper\n",
    "        # attention: [n_batch, n_head, n_Q, n_K]\n",
    "        # x_raw: [n_batch, n_head, n_Q, d_head]\n",
    "\n",
    "        # Reshape x\n",
    "        # Reshape를 하는 이유는 뒤에서 Linear layer를 할 때 필요함\n",
    "        # 차원 수정만 해줌\n",
    "        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()\n",
    "        # x_rsh1: [n_batch, n_Q, n_head, d_head]\n",
    "        # 원래 d_feat으로 복원함\n",
    "        x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat)\n",
    "        # x_rsh2: [n_batch, n_Q, d_feat]\n",
    "\n",
    "        # Linear\n",
    "        x = self.lin_O(x_rsh2)\n",
    "        # x: [n_batch, n_Q, d_feat]\n",
    "        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,\n",
    "               'Q_split':Q_split,'K_split':K_split,'V_split':V_split,\n",
    "               'scores':scores,'attention':attention,\n",
    "               'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x}\n",
    "        return out\n",
    "\n",
    "# Self-Attention Layer\n",
    "n_batch = 128\n",
    "n_src   = 32    # 32개의 word가 한번에 들어가서 32개의 sequence를 처리하겠다.\n",
    "d_feat  = 200\n",
    "n_head  = 5     # 5개의 멀티 헤드 어텐션\n",
    "src = torch.rand(n_batch,n_src,d_feat)\n",
    "self_attention = MultiHeadedAttention(\n",
    "    d_feat=d_feat,n_head=n_head,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=device)\n",
    "# self-attention을 사용하니까 src,src,src 같게 들어간다.\n",
    "out = self_attention.forward(src,src,src,mask=None)\n",
    "\n",
    "Q_feat,K_feat,V_feat = out['Q_feat'],out['K_feat'],out['V_feat']\n",
    "Q_split,K_split,V_split = out['Q_split'],out['K_split'],out['V_split']\n",
    "scores,attention = out['scores'],out['attention']\n",
    "x_raw,x_rsh1,x_rsh2,x = out['x_raw'],out['x_rsh1'],out['x_rsh2'],out['x']\n",
    "\n",
    "# Print out shapes\n",
    "def sh(_x): return str(_x.shape)[11:-1] \n",
    "print (\"Input src:\\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(src)))\n",
    "print ()\n",
    "print (\"Q_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(Q_feat)))\n",
    "print (\"K_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(K_feat)))\n",
    "print (\"V_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(V_feat)))\n",
    "print ()\n",
    "print (\"Q_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(Q_split)))\n",
    "print (\"K_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(K_split)))\n",
    "print (\"V_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(V_split)))\n",
    "print ()\n",
    "print (\"scores:   \\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(scores)))\n",
    "print (\"attention:\\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(attention)))\n",
    "print ()\n",
    "print (\"x_raw:    \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(x_raw)))\n",
    "print (\"x_rsh1:   \\t%s  \\t= [n_batch, n_src, n_head, d_head]\"%(sh(x_rsh1)))\n",
    "print (\"x_rsh2:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x_rsh2)))\n",
    "print ()\n",
    "print (\"Output x: \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4. mha.ipynb의 사본",
   "provenance": [
    {
     "file_id": "https://github.com/sjchoi86/upstage-basic-deeplearning/blob/main/notebook/mha.ipynb",
     "timestamp": 1644296591802
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
