{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention\n",
    "- Query, Key, Value 벡터를 사용함\n",
    "- hidden_dim : 하나의 단어에 대한 차원\n",
    "- n_heads : head의 개수\n",
    "- dropout_ratio : 드롭아웃 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 단어(임베딩)의 차원이 헤드의 개수로 나눠질 수 있어야 한다.\n",
    "        assert hidden_dim % n_heads == 0\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수\n",
    "        self.head_dim = hidden_dim // n_heads # 각 헤드(head)에서의 임베딩 차원\n",
    "        \n",
    "        # Q,K,V 값에 적용될 FC 레이어\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim) # 나중에 추가될 레이어(논문상 구현)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio) # 드롭아웃 비율\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device) # 차원에 루트를 씌움(스케일용)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None): # Q,K,V를 입력 받음\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_q(key)\n",
    "        V = self.fc_q(value)\n",
    "        \n",
    "        # 임베딩의 차원인 hidden_dim을 헤드의 개수로 쪼개준다\n",
    "        # hidden_dim -> n_heads * head_dim\n",
    "        # n_heads개의 서로 다른 어텐션을 학습하도록 한다.\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Score 계산\n",
    "        # Q와 K의 행렬곱이 가능하도록 K에 permute를 수행\n",
    "        score = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        # 마스크 사용하는 경우\n",
    "        if mask is not None:\n",
    "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
    "            score = score.masked_fill(mask==0, -1e10)\n",
    "            \n",
    "        # Attention 계산\n",
    "        # dim=-1 을 통해 열 방향으로 softmax를 수행함\n",
    "        attention = torch.softmax(score, dim=-1)\n",
    "        \n",
    "        # 최종 인코딩된 벡터가 나옴\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        \n",
    "        # 원래 hidden_dim으로 복구\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.hidden_dim)\n",
    "        x = self.fc_o(x)\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feedforward\n",
    "- 입력과 출력의 차원이 동일함\n",
    "- hidden_dim : 하나의 단어(임베딩)에 대한 차원\n",
    "- pf_dim : Feedforward 레이어에서의 내부 임베딩 차원\n",
    "- dropout_ratio: 드롭아웃 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFL(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
    "        super.__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x의 차원: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer\n",
    "- 입력과 출력의 차원이 같다.\n",
    "- 위에서 정의한 멀티헤드어텐션과 Feedforward 클래스를 사용함\n",
    "- Encoder layer를 중첩하면 트랜스포머의 인코더가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim) # 멀티헤드어텐션 다음의 layernorm\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim) # Feedforward 다음의 layernorm\n",
    "        \n",
    "        self.self_attention = MHA(hidden_dim, n_heads, dropout_ratio, device) # 멀티헤드어텐션\n",
    "        self.positionwise_feedforward = PFL(hidden_dim, pf_dim, dropout_ratio) # Feedforward 레이어\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src 차원 : [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        # Attention 수행한 뒤의 인코딩된 벡터 가져옴\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask) # Q,K,V,mask 입력\n",
    "        \n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src)) # residual connection\n",
    "        \n",
    "        # Feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        src = self.ff_layer_norm(src + self.dropout(_src)) # residual connection\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- input_dim: 하나의 단어에 대한 원 핫 인코딩 차원\n",
    "- hidden_dim: 하나의 단어에 대한 임베딩 차원\n",
    "- n_layers: 내부적으로 사용할 인코더 레이어의 개수\n",
    "- n_heads: 헤드(head)의 개수 = scaled dot-product attention의 개수\n",
    "- pf_dim: Feedforward 레이어에서의 내부 임베딩 차원\n",
    "- dropout_ratio: 드롭아웃(dropout) 비율\n",
    "- max_length: 문장 내 최대 단어 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim) # 위치 임베 딩\n",
    "        \n",
    "        # 위에서 정의한 인코더 레이어를 쌓는다.\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src의 차원: [batch_size, src_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        # 0부터 가장 긴 문장에 해당하는 번호까지 들어갈 수 있게 만들고\n",
    "        # 각 문장마다 적용하기 위해 repeat을 수행\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch, 1).to(self.device)\n",
    "        # pos: [batch_size, src_len]\n",
    "        \n",
    "        # src 문장의 임베딩과 pos 임베딩을 더함\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        # src: [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        \n",
    "        # 마지막 레이어의 출력\n",
    "        return src "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
